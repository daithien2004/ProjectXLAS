# -*- coding: utf-8 -*-
import streamlit as st
import cv2
import numpy as np
import tensorflow as tf
from keras.layers import LeakyReLU
import tempfile
from streamlit_webrtc import webrtc_streamer, WebRtcMode, VideoProcessorBase
import av
import time
import os

class EmotionRecognizer:
    def __init__(self):
        """Kh·ªüi t·∫°o m√¥ h√¨nh nh·∫≠n di·ªán c·∫£m x√∫c."""
        self.class_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
        self.emoji_dict = {
            'angry': 'üò†', 'disgust': 'ü§¢', 'fear': 'üò®', 'happy': 'üòÑ',
            'neutral': 'üòê', 'sad': 'üò¢', 'surprise': 'üò≤'
        }
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
        if self.face_cascade.empty():
            st.error("Kh√¥ng th·ªÉ t·∫£i Haar Cascade classifier!")
        self.model = self.load_model()

    @st.cache_resource
    def load_model(_self):
        """T·∫£i m√¥ h√¨nh nh·∫≠n di·ªán c·∫£m x√∫c."""
        model_path = "emotion_model_simple.h5"  # Gi·ªØ nh∆∞ b·∫°n cung c·∫•p, x√°c nh·∫≠n file ·ªü th∆∞ m·ª•c g·ªëc
        if not os.path.exists(model_path):
            st.error(f"Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh: {model_path}")
            return None
        try:
            return tf.keras.models.load_model(model_path, custom_objects={'LeakyReLU': LeakyReLU})
        except Exception as e:
            st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh: {str(e)}")
            return None

    def preprocess_image(self, image):
        """X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o cho m√¥ h√¨nh."""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            resized = cv2.resize(gray, (48, 48))
            normalized = resized / 255.0
            return normalized.reshape(1, 48, 48, 1)
        except Exception as e:
            st.error(f"L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}")
            return None

    def display_prediction(self, prediction, placeholder):
        """Hi·ªÉn th·ªã danh s√°ch 7 c·∫£m x√∫c v√† highlight c·∫£m x√∫c n·ªïi b·∫≠t"""
        placeholder.empty()
        
        if prediction is None:
            with placeholder:
                st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t!")
            return
        
        with placeholder.container():
            # T√¨m c·∫£m x√∫c c√≥ x√°c su·∫•t cao nh·∫•t
            top_index = np.argmax(prediction[0])
            top_emotion = self.class_labels[top_index]
            top_prob = prediction[0][top_index] * 100
            
            # Hi·ªÉn th·ªã danh s√°ch 7 c·∫£m x√∫c
            st.markdown("### üìä X√°c su·∫•t c√°c c·∫£m x√∫c:")
            
            # T·∫°o 2 c·ªôt ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp h∆°n
            col1, col2 = st.columns(2)
            
            for i, label in enumerate(self.class_labels):
                prob = prediction[0][i] * 100
                emoji = self.emoji_dict[label]
                
                # Highlight c·∫£m x√∫c c√≥ x√°c su·∫•t cao nh·∫•t
                if i == top_index:
                    display_text = f"""
                    <div style='background-color:#f0f8ff; padding:8px; border-radius:5px; 
                                border-left:4px solid #1E90FF; margin-bottom:5px;'>
                        {emoji} <strong>{label.capitalize()}</strong>: 
                        <span style='color:#1E90FF;'>{prob:.2f}% ‚òÖ</span>
                    </div>
                    """
                else:
                    display_text = f"{emoji} {label.capitalize()}: `{prob:.2f}%`"
                
                # Ph√¢n b·ªë ƒë·ªÅu v√†o 2 c·ªôt
                if i < 4:  # 3 c·∫£m x√∫c ƒë·∫ßu hi·ªÉn th·ªã ·ªü c·ªôt 1
                    col1.markdown(display_text, unsafe_allow_html=True)
                else:       # 4 c·∫£m x√∫c sau hi·ªÉn th·ªã ·ªü c·ªôt 2
                    col2.markdown(display_text, unsafe_allow_html=True)
            
            # Hi·ªÉn th·ªã c·∫£m x√∫c n·ªïi b·∫≠t b√™n d∆∞·ªõi
            st.markdown("---")
            st.markdown(
                f"""
                <div style='text-align: center; background-color:#f0f8ff; padding:15px; 
                            border-radius:10px; margin-top:15px;'>
                    <h3>üéØ C·∫¢M X√öC N·ªîI B·∫¨T</h3>
                    <div style='font-size:24px;'>
                        {self.emoji_dict[top_emotion]} <strong>{top_emotion.upper()}</strong> 
                        - <span style='color:#1E90FF;'>{top_prob:.2f}%</span>
                    </div>
                </div>
                """,
                unsafe_allow_html=True
            )

    def process_frame(self, frame):
        """X·ª≠ l√Ω m·ªôt khung h√¨nh ƒë·ªÉ nh·∫≠n di·ªán c·∫£m x√∫c."""
        img = frame.copy()
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)
        prediction = None

        if len(faces) == 0:
            return img, None

        for (x, y, w, h) in faces:
            face = img[y:y+h, x:x+w]
            input_face = self.preprocess_image(face)
            if input_face is not None:
                prediction = self.model.predict(input_face, verbose=0)
                label = self.class_labels[np.argmax(prediction)]
                cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
                cv2.putText(
                    img,
                    label.capitalize(),
                    (x, y-10),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    0.8,
                    (0, 255, 0),
                    2
                )

        return img, prediction

    def process_webcam(self, rtc_configuration):
        """X·ª≠ l√Ω video t·ª´ webcam."""
        class EmotionProcessor(VideoProcessorBase):
            def __init__(self):
                self.recognizer = EmotionRecognizer()
                self.latest_prediction = None

            def recv(self, frame):
                img = frame.to_ndarray(format="bgr24")
                processed_img, prediction = self.recognizer.process_frame(img)
                self.latest_prediction = prediction
                return av.VideoFrame.from_ndarray(processed_img, format="bgr24")

        # Hi·ªÉn th·ªã giao di·ªán
        col1, col2 = st.columns([1, 3])
        with col1:
            result_placeholder = st.empty()
        with col2:
            # Kh·ªüi t·∫°o WebRTC trong c·ªôt 2
            ctx = webrtc_streamer(
                key="emotion-webcam",
                mode=WebRtcMode.SENDRECV,
                rtc_configuration=rtc_configuration,
                video_processor_factory=EmotionProcessor,
                async_processing=True,
                media_stream_constraints={"video": True, "audio": False}
            )

            # V√≤ng l·∫∑p c·∫≠p nh·∫≠t UI
            while ctx.state.playing:
                if ctx.video_processor and ctx.video_processor.latest_prediction is not None:
                    # Hi·ªÉn th·ªã k·∫øt qu·∫£
                    self.display_prediction(
                        ctx.video_processor.latest_prediction,
                        result_placeholder
                    )
                time.sleep(0.05)  # Gi·∫£m CPU usage

    def process_video(self, video_path, is_uploaded=False):
        """X·ª≠ l√Ω video t·ª´ file m·∫´u ho·∫∑c upload."""
        if self.model is None:
            st.error("Kh√¥ng th·ªÉ ch·∫°y video v√¨ m√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c t·∫£i!")
            return

        if is_uploaded:
            tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
            tfile.write(video_path.read())
            tfile.close()
            video_path = tfile.name
        else:
            if not os.path.exists(video_path):
                st.error(f"File video '{video_path}' kh√¥ng t·ªìn t·∫°i!")
                return

        st.video(video_path, autoplay=False)

        if "video_running" not in st.session_state:
            st.session_state.video_running = False

        col1, col2 = st.columns([1, 3])
        with col1:
            if st.button("‚ñ∂Ô∏è B·∫Øt ƒë·∫ßu nh·∫≠n d·∫°ng"):
                st.session_state.video_running = True
            if st.button("‚èπÔ∏è D·ª´ng l·∫°i"):
                st.session_state.video_running = False
        with col2:
            frame_window = st.image([])

        prob_placeholder = col1.empty()
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            st.error(f"Kh√¥ng th·ªÉ m·ªü file video: {video_path}")
            if is_uploaded:
                time.sleep(0.5)
                try:
                    os.remove(video_path)
                except PermissionError:
                    st.warning("Kh√¥ng th·ªÉ x√≥a file video t·∫°m th·ªùi v√¨ n√≥ ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng.")
            return

        last_update = 0
        while cap.isOpened() and st.session_state.video_running:
            ret, frame = cap.read()
            if not ret:
                break
            frame, prediction = self.process_frame(frame)
            frame_window.image(frame, channels="BGR")
            if prediction is not None:
                current_time = time.time()
                if current_time - last_update >= 0.2:
                    self.display_prediction(prediction, prob_placeholder)
                    last_update = current_time
            time.sleep(0.05)

        cap.release()
        time.sleep(0.5)
        if is_uploaded:
            for _ in range(3):
                try:
                    os.remove(video_path)
                    break
                except PermissionError:
                    time.sleep(1)
                    st.warning("ƒêang th·ª≠ x√≥a file video t·∫°m th·ªùi...")
            else:
                st.warning("Kh√¥ng th·ªÉ x√≥a file video t·∫°m th·ªùi v√¨ n√≥ ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng.")

    def process_image(self):
        """X·ª≠ l√Ω ·∫£nh upload."""
        if self.model is None:
            st.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh v√¨ m√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c t·∫£i!")
            return

        uploaded = st.file_uploader("T·∫£i l√™n ·∫£nh", type=["jpg", "jpeg", "png"])
        if uploaded:
            img = cv2.imdecode(np.frombuffer(uploaded.read(), np.uint8), cv2.IMREAD_COLOR)
            st.image(img, caption="·∫¢nh ƒë√£ t·∫£i l√™n", channels="BGR")
            input_img = self.preprocess_image(img)
            if input_img is not None:
                prediction = self.model.predict(input_img, verbose=0)
                prob_placeholder = st.empty()
                self.display_prediction(prediction, prob_placeholder)
            else:
                st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t trong ·∫£nh!")